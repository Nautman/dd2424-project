{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (2.2.0.post100)\n",
      "Requirement already satisfied: torchvision in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (0.15.2a0)\n",
      "Requirement already satisfied: filelock in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from requests->torchvision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms \n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tempfile import TemporaryDirectory\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import os, time\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, weights_path, device_type='cpu'):\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = 'cuda:0'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = 'mps'\n",
    "\n",
    "    device = torch.device(device_type)\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_data:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += torch.sum(predicted == labels.data)\n",
    "        print('Time taken:', time.time() - start)\n",
    "        print(f'Accuracy of the network on the test images: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, suffix, scheduler=0, num_epochs=25, device_type='cpu'):\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = 'cuda:0'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = 'mps'\n",
    "\n",
    "    device = torch.device(device_type)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(\"./\", suffix + '.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                model.train() if phase == 'train' else model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                print('phase:', phase)\n",
    "                \n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs) # forward pass\n",
    "\n",
    "                        # outputs is probability of each class\n",
    "                        # labels needs to be probability of each class\n",
    "\n",
    "                        # we need the one-hot for the labels to get the loss\n",
    "                        true_outputs = torch.nn.functional.one_hot(labels, num_classes=37).float()\n",
    "\n",
    "                        # create preds by thresholding outputs\n",
    "                        loss = criterion(outputs, true_outputs)\n",
    "\n",
    "                        preds = torch.argmax(outputs, 1)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "                \n",
    "                outputstr = f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}'\n",
    "                print(outputstr)\n",
    "                # f.write(outputstr + '\\n')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "            print('epoch took:', time.time() - start)\n",
    "            start = time.time()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return best_model_params_path, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(\n",
    "        # 70% train, 20% validation, 10% test\n",
    "        conf, train_split=0.7, val_split=0.2\n",
    "    ):\n",
    "\n",
    "    base_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    "\n",
    "    training_transforms = []\n",
    "\n",
    "    crop = conf.get('crop', False)\n",
    "    random_flip = conf.get('random_flip', False)\n",
    "    rotate_angle = conf.get('rotate_angle', 0)\n",
    "    scale = conf.get('scale', 1)\n",
    "\n",
    "    if crop:\n",
    "        base_transforms.append(transforms.Resize(256))\n",
    "        base_transforms.append(transforms.CenterCrop(224))\n",
    "    else:\n",
    "        base_transforms.append(transforms.Resize(224))\n",
    "    \n",
    "\n",
    "    if random_flip:\n",
    "        training_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    if rotate_angle != 0:\n",
    "        training_transforms.append(transforms.RandomRotation(rotate_angle))\n",
    "\n",
    "    if scale != 1:\n",
    "        training_transforms.append(transforms.Resize(int(224 * scale)))\n",
    "\n",
    "    dataset=OxfordIIITPet(root=\"./\", download=True, target_types='category', transform=transforms.Compose(base_transforms))\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_dataset = dataset\n",
    "    train_size = int(train_split * len(train_dataset))\n",
    "    val_size = int(val_split * len(train_dataset))\n",
    "    test_size = len(train_dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "    # Transform the training dataset\n",
    "    train_dataset.dataset.transform = transforms.Compose(base_transforms + training_transforms)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "        'test': torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True),\n",
    "        'val': torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True),\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        'train': len(train_dataset),\n",
    "        'test': len(test_dataset),\n",
    "        'val': len(val_dataset),\n",
    "    }\n",
    "\n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_resnet(resnet_size = \"18\"):\n",
    "    if resnet_size == \"18\":\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    elif resnet_size == \"50\":\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    for param in resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(resnet.fc)\n",
    "\n",
    "    #resnet fc input size\n",
    "    input_size = resnet.fc.in_features\n",
    "\n",
    "    resnet.fc = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(input_size, 256),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(0.2),\n",
    "        nn.Linear(256, 37),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "    return resnet\n",
    "\n",
    "def multiclass_efficientnet(efficientnet_size = \"b4\"):\n",
    "    if efficientnet_size == \"b4\":\n",
    "        efficientnet = efficientnet_b4(weights=EfficientNetB4_Weights.DEFAULT)\n",
    "    elif efficientnet_size == \"v2_m\":\n",
    "        efficientnet = efficientnet_v2_m(weights=EfficientNetV2M_Weights.DEFAULT)\n",
    "    \n",
    "    for param in efficientnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(efficientnet.classifier)\n",
    "\n",
    "    #efficientnet fc input size\n",
    "    input_size = efficientnet.classifier.in_features\n",
    "\n",
    "    efficientnet.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(input_size, 256),\n",
    "        nn.ReLU(),\n",
    "        # nn.Dropout(0.2),\n",
    "        nn.Linear(256, 37),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "    return efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_suffix(\n",
    "        conf,\n",
    "    ):\n",
    "    model_name = conf.get('model_name', 'resnet50')\n",
    "    crop = conf.get('crop', True)\n",
    "    random_flip = conf.get('random_flip', False)\n",
    "    rotate_angle = conf.get('rotate_angle', 0)\n",
    "    scale = conf.get('scale', 1)\n",
    "    device_type = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = 'cuda:0'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = 'mps'\n",
    "    suffix = model_name\n",
    "    suffix += device_type\n",
    "    if crop:\n",
    "        suffix += \"crop\"\n",
    "    if random_flip:\n",
    "        suffix += \"flip\"\n",
    "    if rotate_angle != 0:\n",
    "        suffix += \"rotate\" + str(rotate_angle)\n",
    "    if scale != 1:\n",
    "        suffix += \"scale\" + str(scale)\n",
    "    return suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet50'\n",
    "random_flip = False\n",
    "rotate_angle = 0\n",
    "crop = True\n",
    "scale = 1\n",
    "\n",
    "base_conf = {\n",
    "    'model_name': model_name,\n",
    "    'crop': crop,\n",
    "    'random_flip': random_flip,\n",
    "    'rotate_angle': rotate_angle,\n",
    "    'scale': scale\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders, dataset_sizes = generate_datasets(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet = multiclass_resnet(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffix = create_suffix(conf)\n",
    "#suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(resnet, dataloaders, dataset_sizes, suffix, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    if model_name == 'resnet18':\n",
    "        return multiclass_resnet(\"18\")\n",
    "    elif model_name == 'resnet50':\n",
    "        return multiclass_resnet(\"50\")\n",
    "    elif model_name == 'efficientnet_b4':\n",
    "        return multiclass_efficientnet(efficientnet_size=\"b4\")\n",
    "    elif model_name == 'efficientnet_v2_m':\n",
    "        return multiclass_efficientnet(efficientnet_size=\"v2_m\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "Epoch 0/4\n",
      "----------\n",
      "phase: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/douglas/anaconda3/envs/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def generate_confs():\n",
    "    models = ['resnet18', 'resnet50', 'efficientnet', 'efficientnet-v2']\n",
    "    crops = [True, False]\n",
    "    random_flips = [True, False]\n",
    "    rotate_angles = [0, 90]\n",
    "    scales = [1, 1.5]\n",
    "\n",
    "    # Generate confs \n",
    "    confs = [\n",
    "        {**base_conf, 'model_name': model}\n",
    "        for model in models\n",
    "    ]\n",
    "\n",
    "    return confs\n",
    "    \n",
    "confs = generate_confs()\n",
    "\n",
    "weights_and_accs = []\n",
    "for conf in confs:\n",
    "    dataloaders, dataset_sizes = generate_datasets(conf)\n",
    "    model = get_model(conf['model_name'])\n",
    "    suffix = create_suffix(conf)\n",
    "    weights_path, best_acc = train_model(model, dataloaders, dataset_sizes, suffix, num_epochs=5)\n",
    "    weights_and_accs.append((weights_path, best_acc))\n",
    "\n",
    "print(weights_and_accs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd2424-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
